from langchain.chat_models.base import BaseChatModel
from langchain.schema import AIMessage, HumanMessage, SystemMessage
from pydantic import Field, BaseModel
import requests
from typing import List, Optional


class CustomLLMChatModel(BaseChatModel):
    api_url: str = Field(..., description="The API URL where the custom LLM is hosted")
    api_key: Optional[str] = Field(None, description="Optional API key for authentication")

    def __init__(self, api_url: str, api_key: Optional[str] = None):
        """
        Initialize the custom chat model with the API URL and optional API key.
        """
        super().__init__(api_url=api_url, api_key=api_key)

    def _generate(self, messages: List[dict], stop: Optional[List[str]] = None) -> str:
        """
        Override the _generate method to send requests to the custom API.

        Args:
            messages (List[dict]): List of messages to send to the API.
            stop (Optional[List[str]]): Optional stopping criteria for the LLM.

        Returns:
            str: The model's response from the API.
        """
        headers = {
            'Content-Type': 'application/json',
        }
        if self.api_key:
            headers['Authorization'] = f'Bearer {self.api_key}'

        payload = {
            "messages": messages,  # Pass the message history to the API
            "stop": stop,  # If your API supports stop sequences
        }

        response = requests.post(self.api_url, json=payload, headers=headers)
        response.raise_for_status()  # Raise an error for bad responses
        
        return response.json().get("response", "")

    def _create_messages(self, messages: List) -> List[dict]:
        """
        Convert LangChain messages to a format your API expects.
        """
        api_messages = []
        for message in messages:
            if isinstance(message, HumanMessage):
                api_messages.append({"role": "user", "content": message.content})
            elif isinstance(message, AIMessage):
                api_messages.append({"role": "assistant", "content": message.content})
            elif isinstance(message, SystemMessage):
                api_messages.append({"role": "system", "content": message.content})
        return api_messages

    def generate(self, messages: List) -> AIMessage:
        """
        Public method to send messages to the custom LLM API and return the response as an AIMessage.
        """
        api_messages = self._create_messages(messages)
        response_text = self._generate(api_messages)
        return AIMessage(content=response_text)

# Example usage
custom_llm = CustomLLMChatModel(
    api_url="https://your-custom-api.com/v1/llm",
    api_key="your-api-key"
)

response = custom_llm.generate(messages=[
    HumanMessage(content="What is the capital of France?")
])

print(response.content)
