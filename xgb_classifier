
import xgboost as xgb
from hyperopt import hp, tpe, fmin, Trials, STATUS_OK
from sklearn.model_selection import train_test_split
from sklearn.metrics import log_loss, precision_score, recall_score, confusion_matrix
from sklearn.datasets import load_breast_cancer
from sklearn.utils.class_weight import compute_class_weight
import numpy as np
import matplotlib.pyplot as plt
import itertools

# Load dataset (replace with your data loading code)
data = load_breast_cancer()
X, y = data.data, data.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Calculate class weights
class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
scale_pos_weight = class_weights[1] / class_weights[0]

# Define the hyperparameter search space
space = {
    'max_depth': hp.quniform('max_depth', 3, 12, 1),
    'n_estimators': hp.quniform('n_estimators', 50, 500, 1),
    'learning_rate': hp.loguniform('learning_rate', np.log(1e-3), np.log(1e-1)),
    'subsample': hp.uniform('subsample', 0.5, 1.0),
    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),
    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),
    'gamma': hp.loguniform('gamma', np.log(1e-8), np.log(10.0)),
    'reg_alpha': hp.loguniform('reg_alpha', np.log(1e-8), np.log(10.0)),
    'reg_lambda': hp.loguniform('reg_lambda', np.log(1e-8), np.log(10.0)),
    'scale_pos_weight': hp.choice('scale_pos_weight', [scale_pos_weight]),
    'n_jobs': hp.choice('n_jobs', [-1])
}

# Define the objective function for Hyperopt
def objective(params):
    params['max_depth'] = int(params['max_depth'])
    params['n_estimators'] = int(params['n_estimators'])
    params['min_child_weight'] = int(params['min_child_weight'])

    model = xgb.XGBClassifier(**params, use_label_encoder=False, eval_metric='logloss')
    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=100, verbose=False)

    y_test_proba = model.predict_proba(X_test)[:, 1]
    loss = log_loss(y_test, y_test_proba)
    
    return {'loss': loss, 'status': STATUS_OK}

# Run Hyperopt optimization
trials = Trials()
best = fmin(fn=objective,
            space=space,
            algo=tpe.suggest,
            max_evals=100,
            trials=trials)

# Extract the best hyperparameters
best_params = {
    'max_depth': int(best['max_depth']),
    'n_estimators': int(best['n_estimators']),
    'learning_rate': best['learning_rate'],
    'subsample': best['subsample'],
    'colsample_bytree': best['colsample_bytree'],
    'min_child_weight': int(best['min_child_weight']),
    'gamma': best['gamma'],
    'reg_alpha': best['reg_alpha'],
    'reg_lambda': best['reg_lambda'],
    'scale_pos_weight': scale_pos_weight,
    'n_jobs': -1
}
print("Best hyperparameters: ", best_params)

# Train the final model with the best hyperparameters and class weights
final_model = xgb.XGBClassifier(**best_params, use_label_encoder=False, eval_metric='logloss')
final_model.fit(X_train, y_train)

# Get predicted probabilities
y_test_proba = final_model.predict_proba(X_test)[:, 1]

# Define thresholds
thresholds = np.linspace(0.1, 0.9, 8)

# Evaluate different thresholds and print metrics
for threshold in thresholds:
    y_pred = (y_test_proba >= threshold).astype(int)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    cm = confusion_matrix(y_test, y_pred)
    
    print(f"\nThreshold: {threshold}")
    print(f"Precision: {precision}")
    print(f"Recall: {recall}")
    print(f"Confusion Matrix:\n{cm}")

# Plot feature importance
xgb.plot_importance(final_model, importance_type='weight')
plt.show()
