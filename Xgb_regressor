import xgboost as xgb
from hyperopt import hp, tpe, fmin, Trials, STATUS_OK
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import LabelEncoder
from sklearn.datasets import fetch_california_housing
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Load dataset (replace with your data loading code)
data = fetch_california_housing()
X, y = data.data, data.target

# Example data preparation with two categorical features
# Here we'll create a DataFrame and add two synthetic categorical features for demonstration
X = pd.DataFrame(X, columns=data.feature_names)
X['cat_feature_1'] = np.random.choice(['A', 'B', 'C'], size=len(X))
X['cat_feature_2'] = np.random.choice(['X', 'Y'], size=len(X))

# Label encode the categorical features
label_encoders = {}
for col in ['cat_feature_1', 'cat_feature_2']:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col])
    label_encoders[col] = le

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the hyperparameter search space
space = {
    'max_depth': hp.quniform('max_depth', 3, 12, 1),
    'n_estimators': hp.quniform('n_estimators', 50, 500, 1),
    'learning_rate': hp.loguniform('learning_rate', np.log(1e-3), np.log(1e-1)),
    'subsample': hp.uniform('subsample', 0.5, 1.0),
    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),
    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),
    'gamma': hp.loguniform('gamma', np.log(1e-8), np.log(10.0)),
    'reg_alpha': hp.loguniform('reg_alpha', np.log(1e-8), np.log(10.0)),
    'reg_lambda': hp.loguniform('reg_lambda', np.log(1e-8), np.log(10.0)),
    'n_jobs': hp.choice('n_jobs', [-1])
}

# Define the objective function for Hyperopt
def objective(params):
    params['max_depth'] = int(params['max_depth'])
    params['n_estimators'] = int(params['n_estimators'])
    params['min_child_weight'] = int(params['min_child_weight'])

    model = xgb.XGBRegressor(**params)
    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=100, verbose=False)

    y_test_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_test_pred)
    
    return {'loss': mse, 'status': STATUS_OK}

# Run Hyperopt optimization
trials = Trials()
best = fmin(fn=objective,
            space=space,
            algo=tpe.suggest,
            max_evals=100,
            trials=trials)

# Extract the best hyperparameters
best_params = {
    'max_depth': int(best['max_depth']),
    'n_estimators': int(best['n_estimators']),
    'learning_rate': best['learning_rate'],
    'subsample': best['subsample'],
    'colsample_bytree': best['colsample_bytree'],
    'min_child_weight': int(best['min_child_weight']),
    'gamma': best['gamma'],
    'reg_alpha': best['reg_alpha'],
    'reg_lambda': best['reg_lambda'],
    'n_jobs': -1
}
print("Best hyperparameters: ", best_params)

# Train the final model with the best hyperparameters
final_model = xgb.XGBRegressor(**best_params)
final_model.fit(X_train, y_train)

# Get predictions
y_test_pred = final_model.predict(X_test)

# Evaluate MSE for the final model
final_mse = mean_squared_error(y_test, y_test_pred)
print(f"Final MSE: {final_mse}")

# Plot feature importance
xgb.plot_importance(final_model, importance_type='weight')
plt.show()
##################################################### weighted regressionm ################################################################################

import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
from hyperopt import fmin, tpe, hp, Trials, STATUS_OK

# Define the custom loss function for XGBoost
def custom_xgb_loss(y_pred, y_true):
    weight = np.where(y_true.get_label() <= 60, 2, 1)
    errors = np.abs(y_true.get_label() - y_pred)
    return 'custom_loss', np.mean(weight * errors)

# Objective function for hyperopt
def objective(params):
    model = xgb.XGBRegressor(
        n_estimators=int(params['n_estimators']),
        max_depth=int(params['max_depth']),
        learning_rate=params['learning_rate'],
        subsample=params['subsample'],
        colsample_bytree=params['colsample_bytree']
    )
    model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], eval_metric=custom_xgb_loss, early_stopping_rounds=10, verbose=False)
    preds = model.predict(X_valid)
    loss = mean_absolute_error(y_valid, preds)
    return {'loss': loss, 'status': STATUS_OK}

# Define the search space
space = {
    'n_estimators': hp.quniform('n_estimators', 100, 1000, 10),
    'max_depth': hp.quniform('max_depth', 3, 15, 1),
    'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),
    'subsample': hp.uniform('subsample', 0.6, 1.0),
    'colsample_bytree': hp.uniform('colsample_bytree', 0.6, 1.0)
}

# Load your data
# X, y = load_your_data_here()

# Split the data
X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)

# Run hyperopt
trials = Trials()
best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=50, trials=trials)

# Train final model with the best hyperparameters
best_params = {
    'n_estimators': int(best['n_estimators']),
    'max_depth': int(best['max_depth']),
    'learning_rate': best['learning_rate'],
    'subsample': best['subsample'],
    'colsample_bytree': best['colsample_bytree']
}

final_model = xgb.XGBRegressor(**best_params)
final_model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], eval_metric=custom_xgb_loss, early_stopping_rounds=10, verbose=True)

# Evaluate the final model
preds = final_model.predict(X_valid)
final_mae = mean_absolute_error(y_valid, preds)
print(f'Final MAE: {final_mae}')

