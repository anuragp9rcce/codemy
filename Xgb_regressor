import xgboost as xgb
from hyperopt import hp, tpe, fmin, Trials, STATUS_OK
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.datasets import load_boston
import numpy as np

# Load dataset (replace with your data loading code)
data = load_boston()
X, y = data.data, data.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the hyperparameter search space
space = {
    'max_depth': hp.quniform('max_depth', 3, 12, 1),
    'n_estimators': hp.quniform('n_estimators', 50, 500, 1),
    'learning_rate': hp.loguniform('learning_rate', np.log(1e-3), np.log(1e-1)),
    'subsample': hp.uniform('subsample', 0.5, 1.0),
    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),
    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),
    'gamma': hp.loguniform('gamma', np.log(1e-8), np.log(10.0)),
    'reg_alpha': hp.loguniform('reg_alpha', np.log(1e-8), np.log(10.0)),
    'reg_lambda': hp.loguniform('reg_lambda', np.log(1e-8), np.log(10.0)),
    'n_jobs': hp.choice('n_jobs', [-1])
}

# Define the objective function for Hyperopt
def objective(params):
    params['max_depth'] = int(params['max_depth'])
    params['n_estimators'] = int(params['n_estimators'])
    params['min_child_weight'] = int(params['min_child_weight'])

    model = xgb.XGBRegressor(**params)
    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=100, verbose=False)

    y_test_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_test_pred)
    
    return {'loss': mse, 'status': STATUS_OK}

# Run Hyperopt optimization
trials = Trials()
best = fmin(fn=objective,
            space=space,
            algo=tpe.suggest,
            max_evals=100,
            trials=trials)

# Extract the best hyperparameters
best_params = {
    'max_depth': int(best['max_depth']),
    'n_estimators': int(best['n_estimators']),
    'learning_rate': best['learning_rate'],
    'subsample': best['subsample'],
    'colsample_bytree': best['colsample_bytree'],
    'min_child_weight': int(best['min_child_weight']),
    'gamma': best['gamma'],
    'reg_alpha': best['reg_alpha'],
    'reg_lambda': best['reg_lambda'],
    'n_jobs': -1
}
print("Best hyperparameters: ", best_params)

# Train the final model with the best hyperparameters
final_model = xgb.XGBRegressor(**best_params)
final_model.fit(X_train, y_train)

# Get predictions
y_test_pred = final_model.predict(X_test)

# Evaluate MSE for the final model
final_mse = mean_squared_error(y_test, y_test_pred)
print(f"Final MSE: {final_mse}")

# Plot feature importance
xgb.plot_importance(final_model, importance_type='weight')
plt.show()
